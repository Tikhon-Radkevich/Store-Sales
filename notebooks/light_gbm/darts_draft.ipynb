{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T16:10:12.403286Z",
     "start_time": "2024-10-24T16:10:12.384312Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "id": "997929215ba9afd4",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T16:43:42.162752Z",
     "start_time": "2024-10-24T16:43:42.125911Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from darts import TimeSeries\n",
    "from darts.models import LightGBMModel\n",
    "\n",
    "from storesales.light_gbm.preprocessing import preprocess\n",
    "from storesales.constants import EXTERNAL_TRAIN_PATH, EXTERNAL_TEST_PATH, EXTERNAL_SAMPLE_SUBMISSION_PATH"
   ],
   "id": "3bc8a8e87a096779",
   "outputs": [],
   "execution_count": 73
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T16:10:18.180788Z",
     "start_time": "2024-10-24T16:10:18.140760Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def clipped_rmsle(y_true, y_pred):\n",
    "    y_pred = np.clip(y_pred, 0, None)\n",
    "    return np.sqrt(np.mean(np.square(np.log1p(y_pred) - np.log1p(y_true))))"
   ],
   "id": "4b706fdd8e62af2b",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T16:32:43.394923Z",
     "start_time": "2024-10-24T16:32:41.782759Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_df = pd.read_csv(EXTERNAL_TRAIN_PATH, parse_dates=[\"date\"])\n",
    "test_df = pd.read_csv(EXTERNAL_TEST_PATH, parse_dates=[\"date\"])"
   ],
   "id": "7e1c3cb731b0f89d",
   "outputs": [],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T16:34:35.055719Z",
     "start_time": "2024-10-24T16:34:34.996698Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_end = train_df[\"date\"].max()\n",
    "train_test_split_date = \"2017-05-10\""
   ],
   "id": "f8699e569c1577b9",
   "outputs": [],
   "execution_count": 55
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T16:32:51.848371Z",
     "start_time": "2024-10-24T16:32:43.462720Z"
    }
   },
   "cell_type": "code",
   "source": "preprocessed_df = preprocess(train_df)",
   "id": "1b84b616242280b5",
   "outputs": [],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T16:35:34.224828Z",
     "start_time": "2024-10-24T16:35:34.018476Z"
    }
   },
   "cell_type": "code",
   "source": "preprocessed_df.fillna(0, inplace=True)",
   "id": "ce14eb86b39f108f",
   "outputs": [],
   "execution_count": 60
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T16:35:35.205171Z",
     "start_time": "2024-10-24T16:35:35.124242Z"
    }
   },
   "cell_type": "code",
   "source": "# test_beverages_data_df = test_df[test_df[\"family\"] == \"BEVERAGES\"]",
   "id": "b5a02855bbf01d82",
   "outputs": [],
   "execution_count": 61
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T16:35:35.473122Z",
     "start_time": "2024-10-24T16:35:35.347723Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# beverages_data_df[\"store_to_family\"] = (\n",
    "#     beverages_data_df[[\"store_nbr\", \"family\"]].astype(str).agg(\"-\".join, axis=1)\n",
    "# )\n",
    "\n",
    "train_columns = [\"date\", \"sales\", \"onpromotion\", \"store_nbr\", \"family\"]\n",
    "\n",
    "train_test_df = pd.concat([preprocessed_df, test_df], axis=0, ignore_index=True)\n",
    "train_data = train_test_df[train_columns].copy()\n",
    "\n",
    "# train_data.rename(columns={\"store_to_family\": \"id\", \"date\": \"time\"}, inplace=True)"
   ],
   "id": "aabb4864c8c0f56a",
   "outputs": [],
   "execution_count": 62
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T16:35:36.085240Z",
     "start_time": "2024-10-24T16:35:35.588902Z"
    }
   },
   "cell_type": "code",
   "source": [
    "threshold_date = pd.Timestamp(\"2017-04-01\")\n",
    "\n",
    "min_dates = train_data.groupby([\"family\", \"store_nbr\"])[\"date\"].min().reset_index()\n",
    "valid_groups = min_dates[min_dates[\"date\"] <= threshold_date]\n",
    "\n",
    "train_data = pd.merge(train_data, valid_groups[['family', 'store_nbr']], on=['family', 'store_nbr'], how='inner')"
   ],
   "id": "ac7180b3bcec5b5b",
   "outputs": [],
   "execution_count": 63
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T16:53:25.033033Z",
     "start_time": "2024-10-24T16:53:24.993200Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_series_and_id_dicts(df: pd.DataFrame):\n",
    "    series_dict = {}\n",
    "    series_id_dict = {}\n",
    "    \n",
    "    for family in df[\"family\"].unique():\n",
    "        series = TimeSeries.from_group_dataframe(\n",
    "            df=df[df[\"family\"] == family],\n",
    "            time_col=\"date\",\n",
    "            value_cols=\"sales\",\n",
    "            group_cols=\"store_nbr\",\n",
    "            static_cols=None,\n",
    "        )\n",
    "        series_id = [{\"store_nbr\": s.static_covariates.store_nbr.iloc[0], \"family\": family} for s in series]\n",
    "        series_id_dict[family] = series_id\n",
    "        \n",
    "        series =  [s.with_static_covariates(None) for s in series]\n",
    "        \n",
    "        series_dict[family] = series\n",
    "    \n",
    "    return series_dict, series_id_dict\n",
    "\n",
    "def get_future_covariates_dict(df: pd.DataFrame):\n",
    "    future_dict = {}\n",
    "    \n",
    "    for family in df[\"family\"].unique():\n",
    "        future_covariates = TimeSeries.from_group_dataframe(\n",
    "            df=df[df[\"family\"] == family],\n",
    "            time_col=\"date\",\n",
    "            value_cols=\"onpromotion\",\n",
    "            group_cols=\"store_nbr\",\n",
    "        )\n",
    "        future_dict[family] = future_covariates\n",
    "    \n",
    "    return future_dict\n",
    "\n",
    "def train_test_split(series: dict[str, list[TimeSeries]], split_date: pd.Timestamp):\n",
    "    train_series = {}\n",
    "    # test_series = {}\n",
    "    \n",
    "    for family, series_list in series.items():\n",
    "        train_series[family] = [s.drop_after(split_date) for s in series_list]\n",
    "        # test_series[family] = [s.slice_intersect(pd.date_range(split_date, s.end_time())) for s in series_list]\n",
    "    \n",
    "    return train_series\n",
    "        \n",
    "    \n",
    "# target = list(map(lambda x: x.drop_after(pd.Timestamp(train_test_split_date)), series))\n",
    "\n",
    "# target_id = [{\"store_nbr\": t.static_covariates.store_nbr.iloc[0], \"family\": \"BEVERAGES\"} for t in target]\n",
    "# target =  [t.with_static_covariates(None) for t in target]\n",
    "# target_dict = {\"BEVERAGES\": [t.astype(np.float32) for t in target]}\n",
    "# id_dict = {\"BEVERAGES\": target_id}"
   ],
   "id": "43f4ac33a75f2b99",
   "outputs": [],
   "execution_count": 74
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T16:53:45.922624Z",
     "start_time": "2024-10-24T16:53:25.594494Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data_series_dict, data_series_id_dict = get_series_and_id_dicts(train_data)\n",
    "future_covariates_dict = get_future_covariates_dict(train_data)"
   ],
   "id": "41f33ff658e3cb77",
   "outputs": [],
   "execution_count": 75
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T16:35:58.989182Z",
     "start_time": "2024-10-24T16:35:57.501467Z"
    }
   },
   "cell_type": "code",
   "source": "train_series_dict = train_test_split(data_series_dict, pd.Timestamp(train_test_split_date))",
   "id": "48145a01b53d9e1e",
   "outputs": [],
   "execution_count": 66
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T16:35:59.036249Z",
     "start_time": "2024-10-24T16:35:58.991081Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# future_covariates = TimeSeries.from_group_dataframe(\n",
    "#     df=train_data,\n",
    "#     time_col=\"date\",\n",
    "#     value_cols=\"onpromotion\",\n",
    "#     group_cols=\"store_nbr\",\n",
    "# )\n",
    "# \n",
    "# future_covs = [f.with_static_covariates(None) for f in future_covs]\n",
    "# future_dict = {\"BEVERAGES\": [f.astype(np.float32) for f in future_covs]}"
   ],
   "id": "e04be05964338de5",
   "outputs": [],
   "execution_count": 67
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T16:55:33.136813Z",
     "start_time": "2024-10-24T16:55:18.413371Z"
    }
   },
   "cell_type": "code",
   "source": [
    "models = {}\n",
    "\n",
    "for family, series in data_series_dict.items():\n",
    "    inputs = {\n",
    "        \"series\": [s.drop_after(pd.Timestamp(\"2017-07-10\")) for s in series],\n",
    "        \"future_covariates\": future_covariates_dict[family],\n",
    "    }\n",
    "    models[family] = LightGBMModel(lags=24, lags_future_covariates=(14, 1), force_col_wise=True)\n",
    "    \n",
    "    models[family].fit(**inputs)"
   ],
   "id": "b1b1911688f49b76",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Total Bins 2487\n",
      "[LightGBM] [Info] Number of data points in the train set: 81055, number of used features: 39\n",
      "[LightGBM] [Info] Start training from score 6.600093\n",
      "[LightGBM] [Info] Total Bins 6150\n",
      "[LightGBM] [Info] Number of data points in the train set: 47003, number of used features: 39\n",
      "[LightGBM] [Info] Start training from score 0.724151\n",
      "[LightGBM] [Info] Total Bins 3281\n",
      "[LightGBM] [Info] Number of data points in the train set: 81047, number of used features: 39\n",
      "[LightGBM] [Info] Start training from score 4.016515\n",
      "[LightGBM] [Info] Total Bins 7931\n",
      "[LightGBM] [Info] Number of data points in the train set: 81057, number of used features: 39\n",
      "[LightGBM] [Info] Start training from score 2574.898948\n",
      "[LightGBM] [Info] Total Bins 5625\n",
      "[LightGBM] [Info] Number of data points in the train set: 51940, number of used features: 24\n",
      "[LightGBM] [Info] Start training from score 0.141593\n",
      "[LightGBM] [Info] Total Bins 7545\n",
      "[LightGBM] [Info] Number of data points in the train set: 81057, number of used features: 39\n",
      "[LightGBM] [Info] Start training from score 501.402241\n",
      "[LightGBM] [Info] Total Bins 6345\n",
      "[LightGBM] [Info] Number of data points in the train set: 63815, number of used features: 39\n",
      "[LightGBM] [Info] Start training from score 14.236394\n",
      "[LightGBM] [Info] Total Bins 7545\n",
      "[LightGBM] [Info] Number of data points in the train set: 81057, number of used features: 39\n",
      "[LightGBM] [Info] Start training from score 1158.035049\n",
      "[LightGBM] [Info] Total Bins 8535\n",
      "[LightGBM] [Info] Number of data points in the train set: 81057, number of used features: 39\n",
      "[LightGBM] [Info] Start training from score 770.502449\n",
      "[LightGBM] [Info] Total Bins 7350\n",
      "[LightGBM] [Info] Number of data points in the train set: 81057, number of used features: 39\n",
      "[LightGBM] [Info] Start training from score 286.963926\n",
      "[LightGBM] [Info] Total Bins 6615\n",
      "[LightGBM] [Info] Number of data points in the train set: 81057, number of used features: 39\n",
      "[LightGBM] [Info] Start training from score 185.714645\n",
      "[LightGBM] [Info] Total Bins 6615\n",
      "[LightGBM] [Info] Number of data points in the train set: 81057, number of used features: 39\n",
      "[LightGBM] [Info] Start training from score 169.295754\n",
      "[LightGBM] [Info] Total Bins 8966\n",
      "[LightGBM] [Info] Number of data points in the train set: 81057, number of used features: 39\n",
      "[LightGBM] [Info] Start training from score 4086.780493\n",
      "[LightGBM] [Info] Total Bins 6239\n",
      "[LightGBM] [Info] Number of data points in the train set: 81057, number of used features: 39\n",
      "[LightGBM] [Info] Start training from score 23.133005\n",
      "[LightGBM] [Info] Total Bins 4646\n",
      "[LightGBM] [Info] Number of data points in the train set: 79589, number of used features: 39\n",
      "[LightGBM] [Info] Start training from score 1.303144\n",
      "[LightGBM] [Info] Total Bins 6495\n",
      "[LightGBM] [Info] Number of data points in the train set: 63997, number of used features: 39\n",
      "[LightGBM] [Info] Start training from score 30.488999\n",
      "[LightGBM] [Info] Total Bins 6360\n",
      "[LightGBM] [Info] Number of data points in the train set: 63995, number of used features: 39\n",
      "[LightGBM] [Info] Start training from score 24.096015\n",
      "[LightGBM] [Info] Total Bins 6083\n",
      "[LightGBM] [Info] Number of data points in the train set: 78619, number of used features: 39\n",
      "[LightGBM] [Info] Start training from score 0.959548\n",
      "[LightGBM] [Info] Total Bins 6800\n",
      "[LightGBM] [Info] Number of data points in the train set: 63872, number of used features: 39\n",
      "[LightGBM] [Info] Start training from score 301.736082\n",
      "[LightGBM] [Info] Total Bins 6360\n",
      "[LightGBM] [Info] Number of data points in the train set: 67303, number of used features: 39\n",
      "[LightGBM] [Info] Start training from score 12.121488\n",
      "[LightGBM] [Info] Total Bins 6345\n",
      "[LightGBM] [Info] Number of data points in the train set: 73933, number of used features: 39\n",
      "[LightGBM] [Info] Start training from score 7.970156\n",
      "[LightGBM] [Info] Total Bins 3926\n",
      "[LightGBM] [Info] Number of data points in the train set: 81054, number of used features: 39\n",
      "[LightGBM] [Info] Start training from score 7.725362\n",
      "[LightGBM] [Info] Total Bins 6435\n",
      "[LightGBM] [Info] Number of data points in the train set: 81057, number of used features: 39\n",
      "[LightGBM] [Info] Start training from score 92.456913\n",
      "[LightGBM] [Info] Total Bins 6095\n",
      "[LightGBM] [Info] Number of data points in the train set: 63490, number of used features: 39\n",
      "[LightGBM] [Info] Start training from score 4.430814\n",
      "[LightGBM] [Info] Total Bins 6975\n",
      "[LightGBM] [Info] Number of data points in the train set: 81057, number of used features: 39\n",
      "[LightGBM] [Info] Start training from score 369.142463\n",
      "[LightGBM] [Info] Total Bins 6885\n",
      "[LightGBM] [Info] Number of data points in the train set: 81057, number of used features: 39\n",
      "[LightGBM] [Info] Start training from score 292.478552\n",
      "[LightGBM] [Info] Total Bins 6225\n",
      "[LightGBM] [Info] Number of data points in the train set: 63685, number of used features: 39\n",
      "[LightGBM] [Info] Start training from score 6.521382\n",
      "[LightGBM] [Info] Total Bins 6360\n",
      "[LightGBM] [Info] Number of data points in the train set: 63808, number of used features: 39\n",
      "[LightGBM] [Info] Start training from score 10.618002\n",
      "[LightGBM] [Info] Total Bins 6855\n",
      "[LightGBM] [Info] Number of data points in the train set: 81055, number of used features: 39\n",
      "[LightGBM] [Info] Start training from score 381.193984\n",
      "[LightGBM] [Info] Total Bins 6345\n",
      "[LightGBM] [Info] Number of data points in the train set: 81057, number of used features: 39\n",
      "[LightGBM] [Info] Start training from score 105.115969\n",
      "[LightGBM] [Info] Total Bins 9867\n",
      "[LightGBM] [Info] Number of data points in the train set: 70856, number of used features: 39\n",
      "[LightGBM] [Info] Start training from score 1759.898635\n",
      "[LightGBM] [Info] Total Bins 6315\n",
      "[LightGBM] [Info] Number of data points in the train set: 54558, number of used features: 39\n",
      "[LightGBM] [Info] Start training from score 5.391330\n",
      "[LightGBM] [Info] Total Bins 6240\n",
      "[LightGBM] [Info] Number of data points in the train set: 81047, number of used features: 39\n",
      "[LightGBM] [Info] Start training from score 24.160189\n"
     ]
    }
   ],
   "execution_count": 77
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T16:40:49.000570Z",
     "start_time": "2024-10-24T16:40:48.946531Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def evaluate():\n",
    "    end_test = train_end - pd.DateOffset(days=15)\n",
    "    test_period = pd.date_range(start=train_test_split_date, end=end_test, freq=\"D\")\n",
    "        \n",
    "    losses = []\n",
    "    for family, series in data_series_dict.items():\n",
    "        family_losses = []\n",
    "        for test_date in tqdm(test_period):\n",
    "            inputs = {\n",
    "                \"series\": [s.drop_after(test_date) for s in series],\n",
    "                \"future_covariates\": future_covariates_dict[family]\n",
    "            }\n",
    "            \n",
    "            preds = models[family].predict(n=16, **inputs)\n",
    "            \n",
    "            true_values = [s.slice_intersect(p) for p, s in zip(preds, series)]\n",
    "            \n",
    "            loss = np.mean([clipped_rmsle(t.values(), p.values()) for t, p in zip(true_values, preds)])\n",
    "            \n",
    "            family_losses.append(loss)\n",
    "        \n",
    "        family_loss = np.mean(family_losses)\n",
    "        print(f\"Family: {family}, Loss: {family_loss}\")\n",
    "        losses.append(family_loss)\n",
    "\n",
    "    return np.mean(losses)\n"
   ],
   "id": "9b6bb08e14e1ffa5",
   "outputs": [],
   "execution_count": 71
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T16:43:02.362460Z",
     "start_time": "2024-10-24T16:40:50.067671Z"
    }
   },
   "cell_type": "code",
   "source": "evaluate()",
   "id": "6762db7c62df5831",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 83/83 [00:18<00:00,  4.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Family: AUTOMOTIVE, Loss: 0.5281642619634838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 83/83 [00:17<00:00,  4.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Family: BABY CARE, Loss: 0.3606325200724416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 83/83 [00:17<00:00,  4.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Family: BEAUTY, Loss: 0.4727290957916864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 83/83 [00:18<00:00,  4.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Family: BEVERAGES, Loss: 0.19436634473106135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 83/83 [00:18<00:00,  4.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Family: BOOKS, Loss: 0.3669966828328123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 83/83 [00:18<00:00,  4.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Family: BREAD/BAKERY, Loss: 0.15671234733508216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 83/83 [00:17<00:00,  4.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Family: CELEBRATION, Loss: 0.568761454951625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 31/83 [00:06<00:11,  4.50it/s]\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "execution_count": 72
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Submission",
   "id": "bbc07825e6c9ed3e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T17:02:22.147763Z",
     "start_time": "2024-10-24T17:02:22.104984Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_df_copy = test_df.copy()\n",
    "test_df_copy.sort_values(by=[\"date\"], inplace=True)"
   ],
   "id": "d3514f2369af2f52",
   "outputs": [],
   "execution_count": 86
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T17:02:49.436998Z",
     "start_time": "2024-10-24T17:02:40.967598Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sub_date = pd.Timestamp(train_end)\n",
    "sub_prediction = {}\n",
    "\n",
    "for family, series in tqdm(data_series_dict.items()):\n",
    "    inputs = {\n",
    "        \"series\": [s.drop_after(sub_date) for s in series],\n",
    "        \"future_covariates\": future_covariates_dict[family]\n",
    "    }\n",
    "    prediction = models[family].predict(n=16, **inputs)\n",
    "    \n",
    "    for i, values in enumerate(prediction):\n",
    "        store_nbr = data_series_id_dict[family][i][\"store_nbr\"]\n",
    "        con = (test_df_copy[\"store_nbr\"] == store_nbr) & (test_df_copy[\"family\"] == family)\n",
    "        test_df_copy.loc[con, \"sales\"] = values.values()"
   ],
   "id": "63ae6183f54a91ef",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33/33 [00:08<00:00,  3.92it/s]\n"
     ]
    }
   ],
   "execution_count": 88
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T17:03:05.674622Z",
     "start_time": "2024-10-24T17:03:05.631267Z"
    }
   },
   "cell_type": "code",
   "source": "test_df_copy.fillna(0, inplace=True)",
   "id": "a7b4c6e0871fe101",
   "outputs": [],
   "execution_count": 90
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T17:03:52.703262Z",
     "start_time": "2024-10-24T17:03:52.666352Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "from storesales.constants import SUBMISSIONS_PATH"
   ],
   "id": "27e2bccbf34fae30",
   "outputs": [],
   "execution_count": 91
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T17:04:58.291401Z",
     "start_time": "2024-10-24T17:04:58.224530Z"
    }
   },
   "cell_type": "code",
   "source": "submission_df = pd.read_csv(EXTERNAL_SAMPLE_SUBMISSION_PATH)",
   "id": "75409e9524c46be9",
   "outputs": [],
   "execution_count": 92
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T17:04:58.866557Z",
     "start_time": "2024-10-24T17:04:58.825746Z"
    }
   },
   "cell_type": "code",
   "source": "submission_df[\"sales\"] = test_df_copy[\"sales\"]",
   "id": "952948c520970ed6",
   "outputs": [],
   "execution_count": 93
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T17:05:24.618058Z",
     "start_time": "2024-10-24T17:05:24.526886Z"
    }
   },
   "cell_type": "code",
   "source": [
    "file_path = os.path.join(SUBMISSIONS_PATH, \"darts_light_gbm_test_submission.csv\")\n",
    "submission_df.to_csv(file_path, index=False)"
   ],
   "id": "546a7ff510005795",
   "outputs": [],
   "execution_count": 96
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "289d99f6940e278d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
